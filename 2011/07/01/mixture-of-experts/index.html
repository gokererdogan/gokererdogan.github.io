<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Mixture of Experts</title>
    <meta name="description" content="Goker Erdogan's personal website.
">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/2011/07/01/mixture-of-experts/">
       <!-- Bootstrap core CSS -->
    <link href="/css/bootstrap.min.flatly.css" rel="stylesheet">
       <!-- Syntax highlighting -->
    <link href="/css/syntax.css" rel="stylesheet">
    <style>
    body {
        font-size: 17px;
        text-align: justify;
    }
    </style>
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-73032750-1', 'auto');
  ga('send', 'pageview');

</script>

</head>


  <body>

    <header class="site-header">
<div class="navbar navbar-default navbar-fixed-top">
      <div class="container" style="max-width: 1000px;">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/index.html">Goker Erdogan</a>
        </div>
        <div class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li><a href="/index.html#about">About</a></li>
            <li><a href="/files/Goker_Erdogan_CV.pdf">CV</a></li>
            <li><a href="/index.html#publications">Publications</a></li>
            <li><a href="/index.html#teaching">Teaching</a></li>
	    <li><a href="https://github.com/gokererdogan">Code</a></li>
            <li><a href="/index.html#posts">Blog</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
</div>
<!--
  <div class="wrapper">

    <a class="site-title" href="/">Goker Erdogan</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

    </nav>

  </div>
-->
</header>




    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Mixture of Experts</h1>
    <p class="post-meta">Jul 1, 2011 â€¢ goker</p>
  </header>

  <article class="post-content" style="font-size: 18px">
    <p><strong>Mixture of experts (MoE)</strong> is a neural network architecture where separate linear models are trained for local regions in input dataset. These linear models are called experts and their outputs are combined with weights given by gating model. Gating model discriminates local regions in data and may employ different functions such as radial basis functions or simple dot product. If data present different characteristics for different regions of input space, one approach would be to run a clustering method on data as a preprocessing step and train separate models for each cluster. MoE architecture couples these clustering and separate model fitting tasks into a single task and may prove to be more successful on certain datasets. We can talk about two types of MoE architecture that differ in their clustering approaches. In <strong>competitive</strong> MoE, local regions in data are forced to focus on discrete sub-spaces in data while in <strong>cooperative</strong> type, there is no enforcement on location of regions and these regions may and most of the time will overlap.</p>

<p>First, I will present a formal definition of MoE architecture and update equations for training such a model. Afterwards, I will provide MoE implementation in MATLAB and illustrate it on a synthetic regression example.</p>

<p>\(X = \lbrace x, r \rbrace\) where \(x\) is a \(d\) x \(N\) matrix where each row vector contains input values and \(r\) is a \(K\) x \(N\) matrix where each row is a vector denoting which class a sample belongs to in classification problems or output values for each output dimension in case of regression.</p>

<p>\(j=1...D\) inputs, \(i=1...K\) outputs, \(h=1...H\) experts, \(\eta\) learning rate<br />
Linear fit of expert h for output i: \(w_{ih} = v_{ih}^{T} x\)<br />
Weight vector of each linear model: This vector is of size D+1 since we add a bias unit. \(v_{ih}\)<br />
Local region center for expert h: \(m_h\) is a D-vector.<br />
Gating Model, weight of expert h: Here dot product is used. \(g_h = \frac{ exp(m_h^T x) }{\Sigma_k exp(m_k^T x)}\)</p>

<p><strong>Regression</strong>:
Output: \(y_i = \Sigma_{h=1}^{H} w_{ih} g_h\)<br />
<strong>Classification</strong>:
Output: \(y_i\) are converted to posterior probabilities through softmax function. \(o_i = \frac{exp(y_i)}{\Sigma_k exp(y_k)}\)</p>

<p>Update equations for regression and classification are derived using gradient ascent. In regression, sum of squares of error is minimized where in classification likelihood of a training instance is maximized. When derived, update equations for regression and classification turn out to be same and given as follows for two types of MoE.</p>

<p><strong>Cooperative MoE</strong>
\(\Delta v_{ih} = \eta (r_i - o_i) g_h x\)
\(\Delta m_h = \eta (r_i - o_i) (w_{ih} - y_i) g_h x\)
( \(o_i = y_i\) for regression )</p>

<p><strong>Competitive MoE</strong>
\(\Delta v_{ih} = \eta f_h (r_i - y_{ih}) x\)
\(\Delta m_h = \eta (f_h - g_h) x\)<br />
For classification, \(f_h = \frac{ g_h exp ( \Sigma_i r_i log y_{ih} )}{\Sigma_k g_k exp ( \Sigma_i r_i log y_{ik} )}\) \(y_{ih} = \frac{exp(w_{ih}}{\Sigma_k exp(w_{kh})}\)<br />
For regression, \(f_h = \frac{ g_h exp ( 0.5 \Sigma_i (r_i - y_{ih})^2 )}{\Sigma_k g_k exp ( 0.5 \Sigma_i (r_i - y_{ik})^2 )}\) \(y_{ih} = w_{ih}\)</p>

<p>For testing my implementation I have generated synthetic regression and classification problems. Below can be seen plot of regression dataset. I have generated points from two linear models by using first linear model in first half of space and second model in the rest also adding some Gaussian noise to samples.</p>

<p><a href="/img/moeinputdata.png"><img src="/img/moeinputdata.png" alt="figInput" /></a></p>

<p>Two functions are implemented for realizing MoE architecture where first one TrainMixtureOfExperts is used to train model and TestMixtureOfExperts is utilized for testing instances on trained MoE network. Below code trains the network on training data and reports error for validation set.</p>

<figure class="highlight"><pre><code class="language-matlab" data-lang="matlab"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre><span class="p">[</span><span class="n">v</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">TrainMixtureOfExperts</span><span class="p">(</span><span class="s1">'regression'</span><span class="p">,</span> <span class="s1">'competitive'</span><span class="p">,</span> <span class="n">tx2</span><span class="p">,</span> <span class="n">tr2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">);</span>
<span class="p">[</span><span class="n">err</span><span class="p">,</span> <span class="n">cr</span><span class="p">]</span> <span class="o">=</span> <span class="n">TestMixtureOfExperts</span><span class="p">(</span><span class="s1">'regression'</span><span class="p">,</span> <span class="n">vx2</span><span class="p">,</span> <span class="n">vr2</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">vx2</span><span class="p">,</span> <span class="n">cr</span><span class="p">,</span> <span class="s1">'xr'</span><span class="p">)</span>
<span class="nb">hold</span> <span class="n">on</span>
<span class="nb">plot</span><span class="p">(</span><span class="n">vx2</span><span class="p">,</span> <span class="n">vr2</span><span class="p">,</span> <span class="s1">'ob'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure>

<p>It can also be seen in the below plot that MoE model learns the underlying model that generates quite successfully by analyzing the predicted outputs on given outputs visually.</p>

<p><a href="/img/moefittedmodel.png"><img src="/img/moefittedmodel.png" alt="figFittedModel" /></a></p>

<p>All source code; MoE functions and synthetic data generation routines can be downloaded from <a href="/files/MoE.rar">here</a>.</p>


  </article>

</div>
<script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">


    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Goker Erdogan</li>
          <!-- <li><a href="mailto:gokererdogan@gmail.com">gokererdogan@gmail.com</a></li> -->
        </ul>
      </div>

      <div class="footer-col  footer-col-2" style="float: right;">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/gokererdogan">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">gokererdogan</span>
            </a>
          </li>
          

          
        </ul>
      </div>
    </div>

  </div>

</footer>
<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="/js/jquery.js"></script>
<script src="/js/bootstrap.min.js"></script>


  </body>

</html>
